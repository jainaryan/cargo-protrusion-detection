{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-25T06:58:42.561941300Z",
     "start_time": "2023-11-25T06:58:34.199519300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "from super_gradients.training import models\n",
    "from tqdm.auto import tqdm\n",
    "from super_gradients.training.utils.distributed_training_utils import setup_device\n",
    "DEVICE = \"cpu\"\n",
    "setup_device(device=DEVICE)\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "shutil.rmtree('./inference_results/images/')\n",
    "os.makedirs('./inference_results/images/', exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T06:58:42.581146700Z",
     "start_time": "2023-11-25T06:58:42.565484700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ROOT_DIR = ''\n",
    "train_imgs_dir = 'data/annotated data/model-data/train/images'\n",
    "train_labels_dir = 'data/annotated data/model-data/train/txt'\n",
    "val_imgs_dir = 'data/annotated data/model-data/val/images'\n",
    "val_labels_dir = 'data/annotated data/model-data/val/txt'\n",
    "test_imgs_dir = 'data/annotated data/model-data/test/images'\n",
    "test_labels_dir = 'data/annotated data/model-data/test/txt'\n",
    "classes = ['protrusion']\n",
    "\n",
    "dataset_params = {\n",
    "    'data_dir':ROOT_DIR,\n",
    "    'train_images_dir':train_imgs_dir,\n",
    "    'train_labels_dir':train_labels_dir,\n",
    "    'val_images_dir':val_imgs_dir,\n",
    "    'val_labels_dir':val_labels_dir,\n",
    "    'test_images_dir':test_imgs_dir,\n",
    "    'test_labels_dir':test_labels_dir,\n",
    "    'classes':classes\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T06:58:42.646206300Z",
     "start_time": "2023-11-25T06:58:42.581146700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "images_for_inference = './data/annotated data/images-to-test-model'\n",
    "images_directory = images_for_inference\n",
    "all_images = os.listdir(images_directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T06:58:42.646206300Z",
     "start_time": "2023-11-25T06:58:42.599907Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "best_model = models.get('yolo_nas_s',\n",
    "                        num_classes=len(dataset_params['classes']),\n",
    "                        checkpoint_path='./checkpoints/yolo_nas_s/RUN_20231125_123948_095631/ckpt_best.pth'\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T10:24:36.669564400Z",
     "start_time": "2023-11-25T10:24:33.921300900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/25 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6896044f34c446f0934ea6a004957d5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image in tqdm(all_images, total=len(all_images)):\n",
    "    image_path = os.path.join(images_directory, image)\n",
    "    out = best_model.predict(image_path)\n",
    "    out.save('./inference_results/images/')\n",
    "    os.rename(\n",
    "        './inference_results/images/pred_0.jpg',\n",
    "        os.path.join('./inference_results/images/', image)\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T10:24:55.632444200Z",
     "start_time": "2023-11-25T10:24:40.326665700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Video: 100%|██████████| 485/485 [02:30<00:00,  3.22it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m video_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/raw data/videos/PXL_20231006_065536169.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m video_predictions \u001B[38;5;241m=\u001B[39m best_model\u001B[38;5;241m.\u001B[39mpredict(video_path)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mvideo_predictions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m video_predictions\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_video.mp4\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\cargo-protrusion-detection\\venv\\lib\\site-packages\\super_gradients\\training\\utils\\predict\\prediction_results.py:557\u001B[0m, in \u001B[0;36mVideoDetectionPrediction.show\u001B[1;34m(self, box_thickness, show_confidence, color_mapping, class_names)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Display the predicted bboxes on the images.\u001B[39;00m\n\u001B[0;32m    549\u001B[0m \n\u001B[0;32m    550\u001B[0m \u001B[38;5;124;03m:param box_thickness:   Thickness of bounding boxes.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;124;03m:param class_names:     List of class names to show. By default, is None which shows all classes using during training.\u001B[39;00m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    556\u001B[0m frames \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdraw(box_thickness\u001B[38;5;241m=\u001B[39mbox_thickness, show_confidence\u001B[38;5;241m=\u001B[39mshow_confidence, color_mapping\u001B[38;5;241m=\u001B[39mcolor_mapping, class_names\u001B[38;5;241m=\u001B[39mclass_names)\n\u001B[1;32m--> 557\u001B[0m \u001B[43mshow_video_from_frames\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDetection\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\cargo-protrusion-detection\\venv\\lib\\site-packages\\super_gradients\\training\\utils\\media\\video.py:177\u001B[0m, in \u001B[0;36mshow_video_from_frames\u001B[1;34m(frames, fps, window_name)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m frames:\n\u001B[0;32m    176\u001B[0m     frame \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGB2BGR)\n\u001B[1;32m--> 177\u001B[0m     \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     cv2\u001B[38;5;241m.\u001B[39mwaitKey(\u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m/\u001B[39m fps))\n\u001B[0;32m    179\u001B[0m cv2\u001B[38;5;241m.\u001B[39mdestroyAllWindows()\n",
      "\u001B[1;31merror\u001B[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "video_path = 'data/raw data/videos/PXL_20231006_065536169.mp4'\n",
    "video_predictions = best_model.predict(video_path)\n",
    "\n",
    "video_predictions.save(\"output_video.mp4\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T12:13:50.385221200Z",
     "start_time": "2023-11-25T12:11:07.339517600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "video_predictions.save(\"output_video.mp4\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T12:16:08.566057100Z",
     "start_time": "2023-11-25T12:15:47.682418800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''images_predictions = out\n",
    "for image_prediction in images_predictions:\n",
    "    class_names = image_prediction.class_names\n",
    "    labels = image_prediction.prediction.labels\n",
    "    confidence = image_prediction.prediction.confidence\n",
    "    bboxes = image_prediction.prediction.bboxes_xyxy\n",
    "\n",
    "    for i, (label, conf, bbox) in enumerate(zip(labels, confidence, bboxes)):\n",
    "        print(\"prediction: \", i)\n",
    "        print(\"label_id: \", label)\n",
    "        print(\"label_name: \", class_names[int(label)])\n",
    "        print(\"confidence: \", conf)\n",
    "        print(\"bbox: \", bbox)\n",
    "        print(\"--\" * 10)'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
    "def yolo2bbox(bboxes):\n",
    "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
    "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
    "    return xmin, ymin, xmax, ymax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_box(image, bboxes, labels):\n",
    "    # Need the image height and width to denormalize\n",
    "    # the bounding box coordinates\n",
    "    height, width, _ = image.shape\n",
    "    lw = max(round(sum(image.shape) / 2 * 0.003), 2)  # Line width.\n",
    "    tf = max(lw - 1, 1) # Font thickness.\n",
    "    for box_num, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = yolo2bbox(box)\n",
    "        # denormalize the coordinates\n",
    "        xmin = int(x1*width)\n",
    "        ymin = int(y1*height)\n",
    "        xmax = int(x2*width)\n",
    "        ymax = int(y2*height)\n",
    "\n",
    "        p1, p2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
    "\n",
    "        class_name = classes[int(labels[box_num])]\n",
    "\n",
    "        color = (0, 0, 255)\n",
    "\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            p1, p2,\n",
    "            color=color,\n",
    "            thickness=lw,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "        # For filled rectangle.\n",
    "        w, h = cv2.getTextSize(\n",
    "            class_name,\n",
    "            0,\n",
    "            fontScale=lw / 3,\n",
    "            thickness=tf\n",
    "        )[0]\n",
    "\n",
    "        outside = p1[1] - h >= 3\n",
    "        new_p2 = p1[0] + w, p2[1] + h + 3 if outside else p2[1] - h - 3\n",
    "\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (p1[0], p2[1]), new_p2,\n",
    "            color=color,\n",
    "            thickness=-1,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            class_name,\n",
    "            (p1[0], p2[1] + h + 2 if outside else p2[1]),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=lw/3,\n",
    "            color=(255, 255, 255),\n",
    "            thickness=tf,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to plot images with the bounding boxes.\n",
    "def plot(image_path, label_path, num_samples):\n",
    "    all_training_images = glob.glob(image_path+'/*')\n",
    "    all_training_labels = glob.glob(label_path+'/*')\n",
    "    all_training_images.sort()\n",
    "    all_training_labels.sort()\n",
    "\n",
    "    temp = list(zip(all_training_images, all_training_labels))\n",
    "    random.shuffle(temp)\n",
    "    all_training_images, all_training_labels = zip(*temp)\n",
    "    all_training_images, all_training_labels = list(all_training_images), list(all_training_labels)\n",
    "\n",
    "    num_images = len(all_training_images)\n",
    "\n",
    "    if num_samples == -1:\n",
    "        num_samples = num_images\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image_name = all_training_images[i].split(os.path.sep)[-1]\n",
    "        image = cv2.imread(all_training_images[i])\n",
    "        with open(all_training_labels[i], 'r') as f:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            label_lines = f.readlines()\n",
    "            for label_line in label_lines:\n",
    "                label, x_c, y_c, w, h = label_line.split(' ')\n",
    "                x_c = float(x_c)\n",
    "                y_c = float(y_c)\n",
    "                w = float(w)\n",
    "                h = float(h)\n",
    "                bboxes.append([x_c, y_c, w, h])\n",
    "                labels.append(label)\n",
    "        result_image = plot_box(image, bboxes, labels)\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        plt.imshow(image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize a few training images.\n",
    "plot(\n",
    "    image_path='./inference_results/images/',\n",
    "    label_path=val_labels_dir,\n",
    "    num_samples=20,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
